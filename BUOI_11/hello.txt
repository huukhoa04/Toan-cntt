minf(x) => x* = argminf(x)
-min <=> max
Gradient Descent Optimization
delta ngược = nabla
gamma nhận gt [0.01, 0.02]
alpha nhận gt 0.05, 0.5
-> thi chạy ko hội tụ -> dùng tham số khác
có Gradient Descent rồi, vì sao lại có momentum:
hàm convex là hàm có 1 VÀ CHỈ 1 giá trị cực tiểu
hàm không lồi có nhiều giá trị cực tiểu và ko có giá trị nhỏ nhất,
hàm lồi chặt: đạo hàm cấp 2 dương: jensen bất đẳng thức.
xk+1 = xk - f(xk)/f'(xk) (Newton)
dùng để giải f(x) = 0
* đối với hàm số có điều kiện:
kết hợp Gradient với jensen
------------------------------
22/4/2024
- Newton
- Gradient Descent
- Gradient Descent & Momentum
------------------------------
1. cho hàm số như sau:
a> trình bày 3 thuật toán để tối ưu f(x)
b> viết ct sử dụng hàm tìm min của f(x) đã cho sử dụng 3 pp đã học với tham số học gradient là gamma, hệ số động lượng là alpha, số bược lặp n và sai số epsilon
x0 = 0, gamma = 0.01(nếu ko hội tụ thì giảm 10 lần gamma), alpha = 0.1, n = 10000, epsilon = 10^-5
ln là hàm log()
- f(x) = 3*e^(x^5 - x^4) + x^2 - 20x + ln(x+25) - 10
- f(x) = 2*e^(x^5 - x^3) - 5x^3 - x + ln(x+45) + 20

- f(xy) = -log10(1/1+e^(-x))         (với y = 1)
        = -log10(1-(1/1+e^(-x)))     (với y = 0)
tìm min với x0 = 5, y0 = 1, gamma = 0.0001, beta = 0.1 
-------------------------
xem test.cpp
